---
title: "Introducing the PatronusAI leaderboard: a leaderboard for real world use-cases"
thumbnail: /blog/assets/leaderboards-on-the-hub/thumbnail.png
authors:
- user: sunitha98
  guest: true
- user: clefourrier
---

# The Patronus AI leaderboard: a leaderboard for real world use-cases
Today, we are excited to announce the new [PatronusAI Leaderboard](https://huggingface.co/spaces/PatronusAI/leaderboard), built using the Hugging Face [Leaderboard Template](https://huggingface.co/demo-leaderboard-backend) in collaboration with their teams. 

The leaderboard aims to evaluate the performance of language models on real-world enterprise use cases. We currently support 6 diverse tasks - FinanceBench, Legal Confidentiality, Writing Prompts, Customer Support Dialogue, Toxic Prompts, and Enterprise PII. 

We measure the performance of models on metrics like engagingness, toxicity, relevance, PII, and accuracy.

<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.45.1/gradio.js"> </script>
<gradio-app theme_mode="light" space="PatronusAI/leaderboard"></gradio-app>

## Why do we need a leaderboard for real world use-cases?
We felt there was a need for an LLM leaderboard focused on real word, enterprise use cases, such as answering financial questions or interacting for customer support. Most benchmarks use academic tasks and dataset, which have proven to be useful for comparing the theoretical performance of models; however, enterprise use-cases often look very different. Our datasets are tailored to look closer to real world use-cases. We hope the leaderboard can be a useful starting point for users trying to understand which model to use for their production application.

There have also been recent [concerns](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/477) about people gaming leaderboards by submitting models fine-tuned on the test sets. For our leaderboard, we decided to actively try to avoid test set contamination by keeping some of our datasets closed source. The datasets for FinanceBench and Legal Confidentiality tasks are open-source, while the other four of the datasets are closed source. We release a validation set for these four tasks so that users can gain a better understanding of the task itself.

## Our Tasks
1. FinanceBench (Islam, Pranab, et al. "FinanceBench: A New Benchmark for Financial Question Answering."): We use 150 prompts to measure the ability of models to answer financial questions given the retrieved context from a document and a question. We evaluate the accuracy of the generated answers.
2. Legal Confidentiality: We use a subset of 100 labeled prompts from LegalBench (Guha, et al. LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models) to measure the ability of LLMs to reason over legal causes. We use few shot prompting and expect the model to respond with a yes/no.
3. Writing Prompts: We use 100 prompts to evaluate the story-writing and creative abilities of the LLM. We measure the engagingness of the text generated by the LLM. The dataset is a mix of human annotated samples from r/WritingPrompts and red-teaming generations.
4. Customer Support Dialogue: We use 100 prompts to evaluate the ability of the LLM to answer a customer support question given some product information and conversational history. We measure if the response was helpful and relevant to the question asked by the customer.
5. Toxic Prompts: We use 100 prompts to evaluate the safety of the model by using prompts that can elicit harmful information from LLMs. We measure if the model generates toxic content.
6. Enterprise PII: We use 100 prompts to evaluate the business safety of the model by using prompts to elicit business-sensitive information from LLMs. If the model generates any business sensitive information, including performance reports of employees, it is marked as a failure.

## Our metrics

## Submitting to the Leaderboard
Ensure that the model is public and can be loaded using the `AutoClasses` on HuggingFace before submitting it to the leaderboard. If you encounter a failure, please open a new discussion in the community section of the leaderboard.

## How to reproduce your results on the validation set

## Contact
If you are interested in automated evaluations for your models, please reach out to us. Our platform allows companies to manage evaluation runs, monitor LLMs in production, and view explanations for failures. We provide custom evaluators and adversarial test suite generators which can be used for any custom test criteria of your choice.
To contact us, please email contact@patronus.ai.