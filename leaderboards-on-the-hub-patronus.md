---
title: "Introducing the PatronusAI leaderboard: a leaderboard for real world use-cases"
thumbnail: /blog/assets/leaderboards-on-the-hub/thumbnail.png
authors:
- user: sunitha98
  guest: true
- user: RebeccaQian1
  guest: true
- user: anandnk24
  guest: true
- user: clefourrier
---
# The Patronus AI leaderboard: a Leaderboard for Real World Use-cases
Today, we are excited to announce the new [PatronusAI Leaderboard](https://huggingface.co/spaces/PatronusAI/leaderboard), built using the Hugging Face [Leaderboard Template](https://huggingface.co/demo-leaderboard-backend) in collaboration with their teams. 
The leaderboard aims to evaluate the performance of language models on real-world enterprise use cases. We currently support 6 diverse tasks - FinanceBench, Legal Confidentiality, Writing Prompts, Customer Support Dialogue, Toxic Prompts, and Enterprise PII. 
We measure the performance of models on metrics like engagingness, toxicity, relevance, PII, and accuracy.
<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.45.1/gradio.js"> </script>
<gradio-app theme_mode="light" space="PatronusAI/leaderboard"></gradio-app>

## Why do we need a leaderboard for real world use-cases?
We felt there was a need for an LLM leaderboard focused on real word, enterprise use cases, such as answering financial questions or interacting for customer support. Most benchmarks use academic tasks and dataset, which have proven to be useful for comparing the theoretical performance of models; however, enterprise use-cases often look very different. Our datasets are tailored to look closer to real world use-cases. We hope the leaderboard can be a useful starting point for users trying to understand which model to use for their production application.
There have also been recent [concerns](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/477) about people gaming leaderboards by submitting models fine-tuned on the test sets. For our leaderboard, we decided to actively try to avoid test set contamination by keeping some of our datasets closed source. The datasets for FinanceBench and Legal Confidentiality tasks are open-source, while the other four of the datasets are closed source. We release a validation set for these four tasks so that users can gain a better understanding of the task itself.

## Our Tasks 
1. [FinanceBench](https://arxiv.org/abs/2311.11944): We use 150 prompts to measure the ability of models to answer financial questions given the retrieved context from a document and a question. To evaluate the accuracy of the responses to FinanceBench task, we use a few shot prompt with gpt-3.5 to evaluate if the generated answer matches our label in free text form.
2. Legal Confidentiality: We use a subset of 100 labeled prompts from LegalBench (Guha, et al. LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models) to measure the ability of LLMs to reason over legal causes. We use few shot prompting and ask the model to respond with a yes/no. We measure the exact match accuracy of the generated output with labels for Legal Confidentiality. 
3. Writing Prompts: We use 100 prompts to evaluate the story-writing and creative abilities of the LLM. The dataset is a mix of human annotated samples from r/WritingPrompts and red-teaming generations. We measure the engagingness of the text generated by the LLM, using the [EnDEX model](https://aclanthology.org/2022.findings-emnlp.359/) model, trained on 80k Reddit-based engagement dataset to evaluate whether the text generated for Writing Prompts is engaging or not.
4. Customer Support Dialogue: We use 100 prompts to evaluate the ability of the LLM to answer a customer support question given some product information and conversational history. For customer support dialogue, we measure if the response was helpful and relevant to the question asked by the customer using few shot prompting with gpt-3.5. The output is marked as irrelevant if it does not directly address the customer's question, provides incomplete information or is not related to the product mentioned in the conversation history.
5. Toxic Prompts: We use 100 prompts to evaluate the safety of the model by using prompts that can elicit harmful information from LLMs. We measure if the model generates text containing rude, disrespectful, or unreasonable comments using the Perspective API.
6. Enterprise PII: We use 100 prompts to evaluate the business safety of the model by using prompts to elicit business-sensitive information from LLMs. If the model generates any business sensitive information, including performance reports of employees, it is marked as a failure. We use a classifier trained on 3000 labeled examples of enterprise-PII to evaluate the generated output.

## Submitting to the Leaderboard
Ensure that the model is public and can be loaded using the `AutoClasses` on HuggingFace before submitting it to the leaderboard. If you encounter a failure, please open a new discussion in the community section of the leaderboard.

## How to view your results on the validation set
While the evaluation code is not open-sourced, the model generations and evaluations on the validation sets will be available [here](https://huggingface.co/datasets/PatronusAI/validation-results) for all the models submitted to the leaderboard.
